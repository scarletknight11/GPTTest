{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-17T16:55:30.810482Z",
     "start_time": "2025-06-17T16:55:30.058137Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(result)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      4\u001B[39m classifier = pipeline(\u001B[33m\"\u001B[39m\u001B[33msentiment-analysis\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m result = \u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mI\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mve been waiting for a HuggingFace course my whole life.\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[38;5;28mprint\u001B[39m(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:159\u001B[39m, in \u001B[36mTextClassificationPipeline.__call__\u001B[39m\u001B[34m(self, inputs, **kwargs)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    125\u001B[39m \u001B[33;03mClassify the text(s) given as inputs.\u001B[39;00m\n\u001B[32m    126\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    156\u001B[39m \u001B[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001B[39;00m\n\u001B[32m    157\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    158\u001B[39m inputs = (inputs,)\n\u001B[32m--> \u001B[39m\u001B[32m159\u001B[39m result = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001B[39;00m\n\u001B[32m    161\u001B[39m _legacy = \u001B[33m\"\u001B[39m\u001B[33mtop_k\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m kwargs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1431\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1423\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1424\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1425\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1428\u001B[39m         )\n\u001B[32m   1429\u001B[39m     )\n\u001B[32m   1430\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1431\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1439\u001B[39m, in \u001B[36mPipeline.run_single\u001B[39m\u001B[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[39m\n\u001B[32m   1437\u001B[39m model_inputs = \u001B[38;5;28mself\u001B[39m.preprocess(inputs, **preprocess_params)\n\u001B[32m   1438\u001B[39m model_outputs = \u001B[38;5;28mself\u001B[39m.forward(model_inputs, **forward_params)\n\u001B[32m-> \u001B[39m\u001B[32m1439\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpostprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1440\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:213\u001B[39m, in \u001B[36mTextClassificationPipeline.postprocess\u001B[39m\u001B[34m(self, model_outputs, function_to_apply, top_k, _legacy)\u001B[39m\n\u001B[32m    209\u001B[39m outputs = model_outputs[\u001B[33m\"\u001B[39m\u001B[33mlogits\u001B[39m\u001B[33m\"\u001B[39m][\u001B[32m0\u001B[39m]\n\u001B[32m    211\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    212\u001B[39m     \u001B[38;5;66;03m# To enable using fp16 and bf16\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m213\u001B[39m     outputs = \u001B[43moutputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    215\u001B[39m     outputs = outputs.numpy()\n",
      "\u001B[31mRuntimeError\u001B[39m: Numpy is not available"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T16:55:01.115739Z",
     "start_time": "2025-06-17T16:55:01.112336Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c695efade636e905",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.1.3\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T16:51:59.423591Z",
     "start_time": "2025-06-17T16:51:59.243080Z"
    }
   },
   "cell_type": "code",
   "source": "classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"])",
   "id": "7a585a5ac1fb5687",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mI\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mve been waiting for a HuggingFace course my whole life.\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mI hate this so much!\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:159\u001B[39m, in \u001B[36mTextClassificationPipeline.__call__\u001B[39m\u001B[34m(self, inputs, **kwargs)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    125\u001B[39m \u001B[33;03mClassify the text(s) given as inputs.\u001B[39;00m\n\u001B[32m    126\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    156\u001B[39m \u001B[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001B[39;00m\n\u001B[32m    157\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    158\u001B[39m inputs = (inputs,)\n\u001B[32m--> \u001B[39m\u001B[32m159\u001B[39m result = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001B[39;00m\n\u001B[32m    161\u001B[39m _legacy = \u001B[33m\"\u001B[39m\u001B[33mtop_k\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m kwargs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1412\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1408\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m can_use_iterator:\n\u001B[32m   1409\u001B[39m     final_iterator = \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   1410\u001B[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[32m   1411\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1412\u001B[39m     outputs = \u001B[38;5;28mlist\u001B[39m(final_iterator)\n\u001B[32m   1413\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n\u001B[32m   1414\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001B[39m, in \u001B[36mPipelineIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[32m    124\u001B[39m item = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterator)\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m processed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    126\u001B[39m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n\u001B[32m    127\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    128\u001B[39m     \u001B[38;5;66;03m# Try to infer the size of the batch\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:213\u001B[39m, in \u001B[36mTextClassificationPipeline.postprocess\u001B[39m\u001B[34m(self, model_outputs, function_to_apply, top_k, _legacy)\u001B[39m\n\u001B[32m    209\u001B[39m outputs = model_outputs[\u001B[33m\"\u001B[39m\u001B[33mlogits\u001B[39m\u001B[33m\"\u001B[39m][\u001B[32m0\u001B[39m]\n\u001B[32m    211\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    212\u001B[39m     \u001B[38;5;66;03m# To enable using fp16 and bf16\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m213\u001B[39m     outputs = \u001B[43moutputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    215\u001B[39m     outputs = outputs.numpy()\n",
      "\u001B[31mRuntimeError\u001B[39m: Numpy is not available"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T16:52:00.730478Z",
     "start_time": "2025-06-17T16:52:00.284675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ],
   "id": "70d6a141a215630f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pipeline\n\u001B[32m      3\u001B[39m classifier = pipeline(\u001B[33m\"\u001B[39m\u001B[33mzero-shot-classification\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      4\u001B[39m                       model=\u001B[33m\"\u001B[39m\u001B[33mdistilbert/distilbert-base-uncased-finetuned-sst-2-english\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mThis is a course about the Transformers library\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcandidate_labels\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meducation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpolitics\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mbusiness\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:206\u001B[39m, in \u001B[36mZeroShotClassificationPipeline.__call__\u001B[39m\u001B[34m(self, sequences, *args, **kwargs)\u001B[39m\n\u001B[32m    203\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    204\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnable to understand extra arguments \u001B[39m\u001B[38;5;132;01m{\u001B[39;00margs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1423\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1421\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[32m   1422\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ChunkPipeline):\n\u001B[32m-> \u001B[39m\u001B[32m1423\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1424\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1425\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   1426\u001B[39m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[32m   1427\u001B[39m             )\n\u001B[32m   1428\u001B[39m         )\n\u001B[32m   1429\u001B[39m     )\n\u001B[32m   1430\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1431\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001B[39m, in \u001B[36mPipelineIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    123\u001B[39m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[32m    124\u001B[39m item = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterator)\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m processed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    126\u001B[39m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n\u001B[32m    127\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    128\u001B[39m     \u001B[38;5;66;03m# Try to infer the size of the batch\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:243\u001B[39m, in \u001B[36mZeroShotClassificationPipeline.postprocess\u001B[39m\u001B[34m(self, model_outputs, multi_label)\u001B[39m\n\u001B[32m    241\u001B[39m sequences = [outputs[\u001B[33m\"\u001B[39m\u001B[33msequence\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m outputs \u001B[38;5;129;01min\u001B[39;00m model_outputs]\n\u001B[32m    242\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m243\u001B[39m     logits = np.concatenate(\u001B[43m[\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogits\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmodel_outputs\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[32m    244\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    245\u001B[39m     logits = np.concatenate([output[\u001B[33m\"\u001B[39m\u001B[33mlogits\u001B[39m\u001B[33m\"\u001B[39m].numpy() \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m model_outputs])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\zero_shot_classification.py:243\u001B[39m, in \u001B[36m<listcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    241\u001B[39m sequences = [outputs[\u001B[33m\"\u001B[39m\u001B[33msequence\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m outputs \u001B[38;5;129;01min\u001B[39;00m model_outputs]\n\u001B[32m    242\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m243\u001B[39m     logits = np.concatenate([\u001B[43moutput\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogits\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m model_outputs])\n\u001B[32m    244\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    245\u001B[39m     logits = np.concatenate([output[\u001B[33m\"\u001B[39m\u001B[33mlogits\u001B[39m\u001B[33m\"\u001B[39m].numpy() \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m model_outputs])\n",
      "\u001B[31mRuntimeError\u001B[39m: Numpy is not available"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.596627Z",
     "start_time": "2025-06-13T15:19:16.123362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model='gpt2')\n",
    "generator(\"In this course, we will teach you how to\")\n"
   ],
   "id": "f6be629236a76c87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to play the game of chess with the help of our chess master.\\n\\nThe course can be completed either through a solo or a group or through the leaderboards of our online chess board games.\\n\\nOnce you have completed the course, you will be able to play the game with friends and your friends will be able to play with you.\\n\\nYou will also learn about the rules of the game and how to play in order to succeed against a real person.\\n\\nTo learn more about the course you can visit our course pages.\\n\\nIf you have any questions, please contact us and we can answer them right away.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.597682900Z",
     "start_time": "2025-06-13T15:19:39.037139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceTB/SmolLM2-360M\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ],
   "id": "e617482f3915dc2f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In this course, we will teach you how to write an essay so that it will be the best it can be. We'll give you tips on how to make the best use of your time in order to finish your project. We'll also show you how to do the research that you need to do in order to support your point of view. We'll also show you how to write an outline for your essay, so that you can organize your thoughts when you are writing. We'll also show you how to write an introduction for an essay, so that your reader will know exactly what the essay is about. And we'll also show you how to write a conclusion for an essay, so that your reader will know exactly what the essay is about.\\n\\nThe best way to learn how to write an essay is to practice. We recommend that you practice by writing several essays of varying lengths. Try to practice writing different types of essays, such as an informative essay, a persuasive essay, a descriptive essay, a narrative essay, or an expository essay. You should also practice writing different types of essays in different genres. For example, you might practice writing an informative essay in the genre of history, or you might practice writing an essay in the genre of poetry. You should write several essays in order to learn how to write\"},\n",
       " {'generated_text': 'In this course, we will teach you how to become a master and discover the power of knowledge through a unique combination of technology, business, and innovation. We will introduce you to the idea of \"knowledge-as-a-resource,\" which means that everything around us—from the books in our homes to the gadgets we use every day—is made up of knowledge. By harnessing the power of knowledge, we can solve problems, create new technologies, and even change the world!\\n\\nLet\\'s start with a fun activity: Close your eyes and imagine that you are an explorer in a magical land where everything around you is made of knowledge. You can see trees, animals, rivers, and mountains made of knowledge. Now think of a problem that you want to solve. Maybe you want to find the best place to build a new store, or maybe you want to learn how to code a video game. Whatever your problem is, you can use the knowledge around you to help you find a solution.\\n\\nIn this course, we will guide you through various activities that will help you develop your knowledge-as-a-resource skills. We will also introduce you to different types of knowledge, such as scientific knowledge, social knowledge, and personal knowledge. By understanding these different kinds of knowledge, you will be better equipped'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.599671Z",
     "start_time": "2025-06-13T15:19:58.202556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unmasker = pipeline(\"fill-mask\", model='distilbert/distilroberta-base')\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)\n"
   ],
   "id": "4523d44e25436025",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19619837403297424,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04052719101309776,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.599671Z",
     "start_time": "2025-06-13T15:19:58.773918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n"
   ],
   "id": "af8f86b705f71528",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "C:\\Users\\sanji\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9981694),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9796019),\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.602389500Z",
     "start_time": "2025-06-13T15:20:10.039264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")"
   ],
   "id": "71bc1ae709f023f8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949769854545593, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.603018Z",
     "start_time": "2025-06-13T15:20:27.103474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of\n",
    "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science. As a result, there\n",
    "    are declining offerings in engineering subjects dealing with infrastructure,\n",
    "    the environment, and related issues, and greater concentration on high\n",
    "    technology subjects, largely supporting increasingly complex scientific\n",
    "    developments. While the latter is important, it should not be at the expense\n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other\n",
    "    industrial countries in Europe and Asia, continue to encourage and advance\n",
    "    the teaching of engineering. Both China and India, respectively, graduate\n",
    "    six and eight times as many traditional engineers as does the United States.\n",
    "    Other industrial countries at minimum maintain their output, while America\n",
    "    suffers an increasingly serious decline in the number of engineering graduates\n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")\n"
   ],
   "id": "3737719a562d09b0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The number of engineering graduates in the United States has declined in recent years . China and India graduate six and eight times as many traditional engineers as the U.S. does . Rapidly developing economies such as China continue to encourage and advance the teaching of engineering . There are declining offerings in engineering subjects dealing with infrastructure, infrastructure, the environment, and related issues .'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.604148100Z",
     "start_time": "2025-06-13T15:20:40.926354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")\n"
   ],
   "id": "c09c26caff7f11ae",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m translator = \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtranslation\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHelsinki-NLP/opus-mt-fr-en\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m translator(\u001B[33m\"\u001B[39m\u001B[33mCe cours est produit par Hugging Face.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1049\u001B[39m, in \u001B[36mpipeline\u001B[39m\u001B[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[39m\n\u001B[32m   1046\u001B[39m             tokenizer_kwargs = model_kwargs.copy()\n\u001B[32m   1047\u001B[39m             tokenizer_kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mtorch_dtype\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m-> \u001B[39m\u001B[32m1049\u001B[39m         tokenizer = \u001B[43mAutoTokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1050\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtokenizer_identifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_fast\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_fast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_from_pipeline\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mtokenizer_kwargs\u001B[49m\n\u001B[32m   1051\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m load_image_processor:\n\u001B[32m   1054\u001B[39m     \u001B[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001B[39;00m\n\u001B[32m   1055\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m image_processor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1037\u001B[39m, in \u001B[36mAutoTokenizer.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[39m\n\u001B[32m   1035\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001B[32m   1036\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1037\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1038\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1039\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33min order to use this tokenizer.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1040\u001B[39m             )\n\u001B[32m   1042\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1043\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m to build an AutoTokenizer.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1044\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mTOKENIZER_MAPPING.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1045\u001B[39m )\n",
      "\u001B[31mValueError\u001B[39m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.605126600Z",
     "start_time": "2025-06-13T15:20:45.001191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "result = image_classifier(\n",
    "        \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    ")\n",
    "\n",
    "print(result)"
   ],
   "id": "5de2c133a80972bb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'lynx, catamount', 'score': 0.43349936604499817}, {'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor', 'score': 0.03479619696736336}, {'label': 'snow leopard, ounce, Panthera uncia', 'score': 0.03240194916725159}, {'label': 'Egyptian cat', 'score': 0.023944800719618797}, {'label': 'tiger cat', 'score': 0.022889256477355957}]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.607111800Z",
     "start_time": "2025-06-13T15:20:51.366382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n",
    "result = transcriber(\n",
    "    \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
    ")\n",
    "print(result)\n"
   ],
   "id": "91001834e59fcc1e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ffmpeg was not found but is required to load audio files from filename",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\audio_utils.py:34\u001B[39m, in \u001B[36mffmpeg_read\u001B[39m\u001B[34m(bpayload, sampling_rate)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43msubprocess\u001B[49m\u001B[43m.\u001B[49m\u001B[43mPopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mffmpeg_command\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstdin\u001B[49m\u001B[43m=\u001B[49m\u001B[43msubprocess\u001B[49m\u001B[43m.\u001B[49m\u001B[43mPIPE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstdout\u001B[49m\u001B[43m=\u001B[49m\u001B[43msubprocess\u001B[49m\u001B[43m.\u001B[49m\u001B[43mPIPE\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m ffmpeg_process:\n\u001B[32m     35\u001B[39m         output_stream = ffmpeg_process.communicate(bpayload)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1022\u001B[39m, in \u001B[36mPopen.__init__\u001B[39m\u001B[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001B[39m\n\u001B[32m   1019\u001B[39m             \u001B[38;5;28mself\u001B[39m.stderr = io.TextIOWrapper(\u001B[38;5;28mself\u001B[39m.stderr,\n\u001B[32m   1020\u001B[39m                     encoding=encoding, errors=errors)\n\u001B[32m-> \u001B[39m\u001B[32m1022\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_execute_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreexec_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1023\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mpass_fds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1024\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshell\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1025\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mp2cread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp2cwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1026\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mc2pread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc2pwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1027\u001B[39m \u001B[43m                        \u001B[49m\u001B[43merrread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1028\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mrestore_signals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1029\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mgid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mumask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1030\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mstart_new_session\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocess_group\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1031\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m   1032\u001B[39m     \u001B[38;5;66;03m# Cleanup if the child failed starting.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1491\u001B[39m, in \u001B[36mPopen._execute_child\u001B[39m\u001B[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001B[39m\n\u001B[32m   1490\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1491\u001B[39m     hp, ht, pid, tid = \u001B[43m_winapi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCreateProcess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexecutable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1492\u001B[39m \u001B[43m                             \u001B[49m\u001B[38;5;66;43;03m# no special security\u001B[39;49;00m\n\u001B[32m   1493\u001B[39m \u001B[43m                             \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1494\u001B[39m \u001B[43m                             \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclose_fds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1495\u001B[39m \u001B[43m                             \u001B[49m\u001B[43mcreationflags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1496\u001B[39m \u001B[43m                             \u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1497\u001B[39m \u001B[43m                             \u001B[49m\u001B[43mcwd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1498\u001B[39m \u001B[43m                             \u001B[49m\u001B[43mstartupinfo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1499\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m   1500\u001B[39m     \u001B[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001B[39;00m\n\u001B[32m   1501\u001B[39m     \u001B[38;5;66;03m# handles that only the child should have open.  You need\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1504\u001B[39m     \u001B[38;5;66;03m# pipe will not close when the child process exits and the\u001B[39;00m\n\u001B[32m   1505\u001B[39m     \u001B[38;5;66;03m# ReadFile will hang.\u001B[39;00m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [WinError 2] The system cannot find the file specified",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m transcriber = pipeline(task=\u001B[33m\"\u001B[39m\u001B[33mautomatic-speech-recognition\u001B[39m\u001B[33m\"\u001B[39m, model=\u001B[33m\"\u001B[39m\u001B[33mopenai/whisper-large-v3\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m result = \u001B[43mtranscriber\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mhttps://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m      4\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:295\u001B[39m, in \u001B[36mAutomaticSpeechRecognitionPipeline.__call__\u001B[39m\u001B[34m(self, inputs, **kwargs)\u001B[39m\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\n\u001B[32m    235\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    236\u001B[39m     inputs: Union[np.ndarray, \u001B[38;5;28mbytes\u001B[39m, \u001B[38;5;28mstr\u001B[39m],\n\u001B[32m    237\u001B[39m     **kwargs,\n\u001B[32m    238\u001B[39m ):\n\u001B[32m    239\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    240\u001B[39m \u001B[33;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001B[39;00m\n\u001B[32m    241\u001B[39m \u001B[33;03m    documentation for more information.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    293\u001B[39m \u001B[33;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001B[39;00m\n\u001B[32m    294\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m295\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1423\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1421\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001B[32m   1422\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.framework == \u001B[33m\"\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ChunkPipeline):\n\u001B[32m-> \u001B[39m\u001B[32m1423\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[32m   1424\u001B[39m         \u001B[38;5;28miter\u001B[39m(\n\u001B[32m   1425\u001B[39m             \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   1426\u001B[39m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[32m   1427\u001B[39m             )\n\u001B[32m   1428\u001B[39m         )\n\u001B[32m   1429\u001B[39m     )\n\u001B[32m   1430\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1431\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001B[39m, in \u001B[36mPipelineIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    121\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_item()\n\u001B[32m    123\u001B[39m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m item = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterator)\n\u001B[32m    125\u001B[39m processed = \u001B[38;5;28mself\u001B[39m.infer(item, **\u001B[38;5;28mself\u001B[39m.params)\n\u001B[32m    126\u001B[39m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001B[39m, in \u001B[36mPipelinePackIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    266\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m accumulator\n\u001B[32m    268\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_last:\n\u001B[32m--> \u001B[39m\u001B[32m269\u001B[39m     processed = \u001B[38;5;28mself\u001B[39m.infer(\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m, **\u001B[38;5;28mself\u001B[39m.params)\n\u001B[32m    270\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    271\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed, torch.Tensor):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    787\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    788\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m789\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    791\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:33\u001B[39m, in \u001B[36m_IterableDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index:\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m         data.append(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset_iter))\n\u001B[32m     34\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m     35\u001B[39m         \u001B[38;5;28mself\u001B[39m.ended = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:186\u001B[39m, in \u001B[36mPipelineChunkIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    183\u001B[39m     \u001B[38;5;28mself\u001B[39m.subiterator = \u001B[38;5;28mself\u001B[39m.infer(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterator), **\u001B[38;5;28mself\u001B[39m.params)\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    185\u001B[39m     \u001B[38;5;66;03m# Try to return next item\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m186\u001B[39m     processed = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msubiterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    187\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    188\u001B[39m     \u001B[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001B[39;00m\n\u001B[32m    189\u001B[39m     \u001B[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    192\u001B[39m     \u001B[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001B[39;00m\n\u001B[32m    193\u001B[39m     \u001B[38;5;66;03m# into a single list, but with generators\u001B[39;00m\n\u001B[32m    194\u001B[39m     \u001B[38;5;28mself\u001B[39m.subiterator = \u001B[38;5;28mself\u001B[39m.infer(\u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterator), **\u001B[38;5;28mself\u001B[39m.params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:375\u001B[39m, in \u001B[36mAutomaticSpeechRecognitionPipeline.preprocess\u001B[39m\u001B[34m(self, inputs, chunk_length_s, stride_length_s)\u001B[39m\n\u001B[32m    372\u001B[39m             inputs = f.read()\n\u001B[32m    374\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(inputs, \u001B[38;5;28mbytes\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m375\u001B[39m     inputs = \u001B[43mffmpeg_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfeature_extractor\u001B[49m\u001B[43m.\u001B[49m\u001B[43msampling_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    377\u001B[39m stride = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    378\u001B[39m extra = {}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\transformers\\pipelines\\audio_utils.py:37\u001B[39m, in \u001B[36mffmpeg_read\u001B[39m\u001B[34m(bpayload, sampling_rate)\u001B[39m\n\u001B[32m     35\u001B[39m         output_stream = ffmpeg_process.communicate(bpayload)\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mffmpeg was not found but is required to load audio files from filename\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merror\u001B[39;00m\n\u001B[32m     38\u001B[39m out_bytes = output_stream[\u001B[32m0\u001B[39m]\n\u001B[32m     39\u001B[39m audio = np.frombuffer(out_bytes, np.float32)\n",
      "\u001B[31mValueError\u001B[39m: ffmpeg was not found but is required to load audio files from filename"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "transcriber = pipeline(\n",
    "    task=\"automatic-speech-recognition\", model=\"openai/whisper-base.en\"\n",
    ")\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n",
    "# Output: {'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}"
   ],
   "id": "3bc8629865084adc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.607712900Z",
     "start_time": "2025-06-05T17:26:04.689644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ],
   "id": "da3934624c331db7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sanji\\PycharmProjects\\HuggingFaceLLM\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "808f16403a5a48759b467541d6743c26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.608787800Z",
     "start_time": "2025-06-13T15:20:59.909967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ],
   "id": "55caeea485ff2cab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.609784400Z",
     "start_time": "2025-06-13T15:35:25.962566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "print(tokenizer)"
   ],
   "id": "cdd6a3c05a184ab1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.610792Z",
     "start_time": "2025-06-13T15:35:26.967317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ],
   "id": "b01d60c73ca1e811",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.610792Z",
     "start_time": "2025-06-13T15:35:27.922394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "print(model)"
   ],
   "id": "338266d8316ab4c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0-5): 6 x TransformerBlock(\n",
      "        (attention): DistilBertSdpaAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.613514300Z",
     "start_time": "2025-06-13T15:35:40.306460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ],
   "id": "2a3992db38d51c81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.614503700Z",
     "start_time": "2025-06-13T16:35:30.474726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ],
   "id": "39e8b595b31abeef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.615517Z",
     "start_time": "2025-06-13T16:35:32.388469Z"
    }
   },
   "cell_type": "code",
   "source": "print(outputs.logits.shape)",
   "id": "76545396788662a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.615517Z",
     "start_time": "2025-06-13T16:35:33.095639Z"
    }
   },
   "cell_type": "code",
   "source": "print(outputs.logits)",
   "id": "908e0d90e9a633b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.617025800Z",
     "start_time": "2025-06-13T16:35:33.684521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ],
   "id": "df8e3960d22ac53c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.619143700Z",
     "start_time": "2025-06-13T16:35:34.346007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = torch.nn.functional.log_softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ],
   "id": "bfb2df86bbcbc4ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.2140e+00, -4.1025e-02],\n",
      "        [-5.4428e-04, -7.5162e+00]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.620124600Z",
     "start_time": "2025-06-13T16:35:34.919636Z"
    }
   },
   "cell_type": "code",
   "source": "model.config.id2label",
   "id": "1f72dc0fddffdb42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.620124600Z",
     "start_time": "2025-06-13T16:35:35.728778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)\n",
    "\n",
    "print(config)"
   ],
   "id": "69b7f3ab6fcf7a10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:51:43.622308300Z",
     "start_time": "2025-06-13T16:35:44.699052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "\n",
    "# Model is randomly initialized!\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n"
   ],
   "id": "989642d62a0cd204",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:52:00.669935Z",
     "start_time": "2025-06-16T17:52:00.629266Z"
    }
   },
   "cell_type": "code",
   "source": "model.save_pretrained(\"directory_on_my_computer\")",
   "id": "1578320409e73340",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mmodel\u001B[49m.save_pretrained(\u001B[33m\"\u001B[39m\u001B[33mdirectory_on_my_computer\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:52:01.887627Z",
     "start_time": "2025-06-16T17:52:01.882658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "print(sequences)"
   ],
   "id": "c64bc5024420667b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'Cool.', 'Nice!']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-16T17:52:03.154494Z",
     "start_time": "2025-06-16T17:52:03.123863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n",
    "model_inputs = torch.tensor(encoded_sequences)\n",
    "output = model(model_inputs)\n",
    "print(output)"
   ],
   "id": "e8437277d7315168",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      1\u001B[39m encoded_sequences = [\n\u001B[32m      2\u001B[39m     [\u001B[32m101\u001B[39m, \u001B[32m7592\u001B[39m, \u001B[32m999\u001B[39m, \u001B[32m102\u001B[39m],\n\u001B[32m      3\u001B[39m     [\u001B[32m101\u001B[39m, \u001B[32m4658\u001B[39m, \u001B[32m1012\u001B[39m, \u001B[32m102\u001B[39m],\n\u001B[32m      4\u001B[39m     [\u001B[32m101\u001B[39m, \u001B[32m3835\u001B[39m, \u001B[32m999\u001B[39m, \u001B[32m102\u001B[39m],\n\u001B[32m      5\u001B[39m ]\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m model_inputs = \u001B[43mtorch\u001B[49m.tensor(encoded_sequences)\n\u001B[32m      7\u001B[39m output = model(model_inputs)\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "\u001B[31mNameError\u001B[39m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T16:37:44.033133Z",
     "start_time": "2025-06-13T16:37:44.030294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ],
   "id": "f732fbbe2d48ef0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T17:57:45.952431Z",
     "start_time": "2025-06-13T17:57:45.415779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer(\"Using a Transformer network is simple\")\n"
   ],
   "id": "5cc1d8d55a2b81a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T17:58:59.899330Z",
     "start_time": "2025-06-13T17:58:59.879164Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.save_pretrained(\"directory_on_my_computer\")",
   "id": "f740afb33b008fd7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('directory_on_my_computer\\\\tokenizer_config.json',\n",
       " 'directory_on_my_computer\\\\special_tokens_map.json',\n",
       " 'directory_on_my_computer\\\\vocab.txt',\n",
       " 'directory_on_my_computer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bfc297f0e4ee4b0e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
